{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PySpark in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\trang\\\\Downloads\\\\spark-3.1.1-bin-hadoop2.7\\\\spark-3.1.1-bin-hadoop2.7'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import PySpark and create the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing data and exploring the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"C:/Users/trang/Downloads/weatherAUS.csv\",format=\"csv\",sep=\",\",inferSchema=\"true\",header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- MinTemp: string (nullable = true)\n",
      " |-- MaxTemp: string (nullable = true)\n",
      " |-- Rainfall: string (nullable = true)\n",
      " |-- Evaporation: string (nullable = true)\n",
      " |-- Sunshine: string (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: string (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: string (nullable = true)\n",
      " |-- WindSpeed3pm: string (nullable = true)\n",
      " |-- Humidity9am: string (nullable = true)\n",
      " |-- Humidity3pm: string (nullable = true)\n",
      " |-- Pressure9am: string (nullable = true)\n",
      " |-- Pressure3pm: string (nullable = true)\n",
      " |-- Cloud9am: string (nullable = true)\n",
      " |-- Cloud3pm: string (nullable = true)\n",
      " |-- Temp9am: string (nullable = true)\n",
      " |-- Temp3pm: string (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RISK_MM: double (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Convert the data types of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data types from pyspark.sql.types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- MinTemp: float (nullable = true)\n",
      " |-- MaxTemp: float (nullable = true)\n",
      " |-- Rainfall: float (nullable = true)\n",
      " |-- Evaporation: float (nullable = true)\n",
      " |-- Sunshine: float (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: float (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: float (nullable = true)\n",
      " |-- WindSpeed3pm: float (nullable = true)\n",
      " |-- Humidity9am: float (nullable = true)\n",
      " |-- Humidity3pm: float (nullable = true)\n",
      " |-- Pressure9am: float (nullable = true)\n",
      " |-- Pressure3pm: float (nullable = true)\n",
      " |-- Cloud9am: float (nullable = true)\n",
      " |-- Cloud3pm: float (nullable = true)\n",
      " |-- Temp9am: float (nullable = true)\n",
      " |-- Temp3pm: float (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RISK_MM: double (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features =[\"MinTemp\",\"MaxTemp\",\"Rainfall\",\"Evaporation\",\"Sunshine\",\n",
    "          \"WindGustSpeed\",\"WindSpeed9am\",\"WindSpeed3pm\",\n",
    "          \"Humidity9am\",\"Humidity3pm\",\"Pressure9am\",\"Pressure3pm\",\n",
    "          \"Cloud9am\",\"Cloud3pm\",\"Temp9am\",\"Temp3pm\"]\n",
    "for each_feature in features:\n",
    "    df =df.withColumn(each_feature,df[each_feature].cast(FloatType()))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Exploring missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinTemp \t with mull values: 637\n",
      "MaxTemp \t with mull values: 322\n",
      "Rainfall \t with mull values: 1406\n",
      "Evaporation \t with mull values: 60843\n",
      "Sunshine \t with mull values: 67816\n",
      "WindGustSpeed \t with mull values: 9270\n",
      "WindSpeed9am \t with mull values: 1348\n",
      "WindSpeed3pm \t with mull values: 2630\n",
      "Humidity9am \t with mull values: 1774\n",
      "Humidity3pm \t with mull values: 3610\n",
      "Pressure9am \t with mull values: 14014\n",
      "Pressure3pm \t with mull values: 13981\n",
      "Cloud9am \t with mull values: 53657\n",
      "Cloud3pm \t with mull values: 57094\n",
      "Temp9am \t with mull values: 904\n",
      "Temp3pm \t with mull values: 2726\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    count = df.filter(df[col].isNull()).count()\n",
    "    if count:\n",
    "        print (col,\"\\t\",\"with mull values:\",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WindGustDir \t with null values: 9330\n",
      "WindDir9am \t with null values: 10013\n",
      "WindDir3pm \t with null values: 3778\n",
      "RainToday \t with null values: 1406\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    count = df.filter(df[col]==\"NA\").count()\n",
    "    if count:\n",
    "        print (col, \"\\t\", \"with null values:\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Drop the feature, RISK_MM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('RISK_MM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Processing the data feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, year, month, dayofmonth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the year, month, day attributes of the converted Date attribute using year(), month() &\n",
    "dayofmonth() and create the corresponding new features Year, Month and Day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"date\", to_date(df['Date']))\n",
    "df = df.withColumn(\"year\", year(df['date']))\n",
    "df = df.withColumn(\"month\", month(df['date']))\n",
    "df = df.withColumn(\"day\", dayofmonth(df['date']))\n",
    "df = df.drop('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Handling missing values of the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|WindGustDir|count|\n",
      "+-----------+-----+\n",
      "|        SSE| 8993|\n",
      "|         SW| 8797|\n",
      "|         NW| 8003|\n",
      "|         NA| 9330|\n",
      "|          E| 9071|\n",
      "|        WSW| 8901|\n",
      "|        ENE| 7992|\n",
      "|         NE| 7060|\n",
      "|        NNW| 6561|\n",
      "|          N| 9033|\n",
      "|        SSW| 8610|\n",
      "|          W| 9780|\n",
      "|          S| 8949|\n",
      "|         SE| 9309|\n",
      "|        WNW| 8066|\n",
      "|        NNE| 6433|\n",
      "|        ESE| 7305|\n",
      "+-----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|WindDir9am|count|\n",
      "+----------+-----+\n",
      "|       SSE| 8966|\n",
      "|        SW| 8237|\n",
      "|        NW| 8552|\n",
      "|        NA|10013|\n",
      "|         E| 9024|\n",
      "|       WSW| 6843|\n",
      "|       ENE| 7735|\n",
      "|        NE| 7527|\n",
      "|       NNW| 7840|\n",
      "|         N|11393|\n",
      "|       SSW| 7448|\n",
      "|         W| 8260|\n",
      "|         S| 8493|\n",
      "|        SE| 9162|\n",
      "|       WNW| 7194|\n",
      "|       NNE| 7948|\n",
      "|       ESE| 7558|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|WindDir3pm|count|\n",
      "+----------+-----+\n",
      "|       SSE| 9142|\n",
      "|        NW| 8468|\n",
      "|        SW| 9182|\n",
      "|        NA| 3778|\n",
      "|         E| 8342|\n",
      "|       WSW| 9329|\n",
      "|       ENE| 7724|\n",
      "|        NE| 8164|\n",
      "|       NNW| 7733|\n",
      "|         N| 8667|\n",
      "|       SSW| 8010|\n",
      "|         W| 9911|\n",
      "|         S| 9598|\n",
      "|        SE|10663|\n",
      "|       WNW| 8656|\n",
      "|       NNE| 6444|\n",
      "|       ESE| 8382|\n",
      "+----------+-----+\n",
      "\n",
      "+---------+------+\n",
      "|RainToday| count|\n",
      "+---------+------+\n",
      "|       NA|  1406|\n",
      "|       No|109332|\n",
      "|      Yes| 31455|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"WindGustDir\").count().show()\n",
    "df.groupBy(\"WindDir9am\").count().show()\n",
    "df.groupBy(\"WindDir3pm\").count().show()\n",
    "df.groupBy(\"RainToday\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the categorical features with missing values (“NA”), replace “NA” with the corresponding most frequenct value of each of the features using when(), list(), otherwise()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "df = df.withColumn(\"WindGustDir\", when(df[\"WindGustDir\"]==\"NA\", lit(\"W\")).otherwise(df[\"WindGustDir\"]))\n",
    "df = df.withColumn(\"WindDir9am\", when(df[\"WindDir9am\"]==\"NA\", lit(\"N\")).otherwise(df[\"WindDir9am\"]))\n",
    "df = df.withColumn(\"WindDir3pm\", when(df[\"WindDir3pm\"]==\"NA\", lit(\"SE\")).otherwise(df[\"WindDir3pm\"]))\n",
    "df = df.withColumn(\"RainToday\", when(df[\"RainToday\"]==\"NA\", lit(\"No\")).otherwise(df[\"RainToday\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Handling missing values of the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm']\n"
     ]
    }
   ],
   "source": [
    "numeric_feats = [item[0] for item in df.dtypes if item[1].startswith('float')]\n",
    "print(numeric_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pyspark.ml.feature.imputer to fill in the missing values of the numerical features with\n",
    "mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "imputer = Imputer(strategy='mean', inputCols=numeric_feats,outputCols=numeric_feats)\n",
    "df = imputer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Transform the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rainfall \t 9.93720720656584\n",
      "Evaporation \t 4.9535525141524825\n",
      "WindGustSpeed \t 0.9042674361671061\n",
      "WindSpeed9am \t 0.7791876023517204\n",
      "['Rainfall', 'Evaporation', 'WindGustSpeed', 'WindSpeed9am']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness\n",
    "skewed_feats = []\n",
    "for col in numeric_feats:\n",
    "    s = df.select(skewness(df[col]).alias(col)).collect()\n",
    "    skew_value = float(s[0][col])\n",
    "    if (skew_value > 0.75):\n",
    "        print (col, \"\\t\", skew_value)\n",
    "        skewed_feats.append(col)\n",
    "print(skewed_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply log transformation those features with skewness values larger than 0.75 using log1p() from\n",
    "pyspark.sql.functions module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import log1p\n",
    "for feat in skewed_feats:\n",
    "    df = df.withColumn(feat, log1p(df[feat]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Converting categorial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']\n"
     ]
    }
   ],
   "source": [
    "categorical_feats = [item[0] for item in df.dtypes if item[1].startswith('string')]\n",
    "categorical_feats.remove('RainTomorrow')\n",
    "print (categorical_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert categorical features into dummy/indicator features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "td = df\n",
    "for feat in categorical_feats:\n",
    "    stringIndexer = StringIndexer(inputCol=feat, outputCol=feat+\"_indexed\")\n",
    "    model = stringIndexer.fit(td)\n",
    "    td = model.transform(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(inputCols=[feat+\"_indexed\" for feat in categorical_feats], outputCols=[feat+\"_ohe\" for feat in categorical_feats])\n",
    "model = encoder.fit(td)\n",
    "encoded = model.transform(td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Training the Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+-------------------+------------------+--------+-----------+------------------+----------+----------+------------------+------------+-----------+-----------+-----------+-----------+---------+---------+-------+-------+---------+------------+----+-----+---+----------------+-------------------+------------------+------------------+-----------------+-------------+---------------+---------------+---------------+---------------+--------------------+\n",
      "|Location|MinTemp|MaxTemp|           Rainfall|       Evaporation|Sunshine|WindGustDir|     WindGustSpeed|WindDir9am|WindDir3pm|      WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm| Cloud9am| Cloud3pm|Temp9am|Temp3pm|RainToday|RainTomorrow|year|month|day|Location_indexed|WindGustDir_indexed|WindDir9am_indexed|WindDir3pm_indexed|RainToday_indexed|RainToday_ohe| WindDir3pm_ohe| WindDir9am_ohe|WindGustDir_ohe|   Location_ohe|            features|\n",
      "+--------+-------+-------+-------------------+------------------+--------+-----------+------------------+----------+----------+------------------+------------+-----------+-----------+-----------+-----------+---------+---------+-------+-------+---------+------------+----+-----+---+----------------+-------------------+------------------+------------------+-----------------+-------------+---------------+---------------+---------------+---------------+--------------------+\n",
      "|  Albury|   13.4|   22.9|0.47000364414689666|1.8671489542210176|7.624853|          W|3.8066624897703196|         W|       WNW| 3.044522437723423|        24.0|       71.0|       22.0|     1007.7|     1007.1|      8.0|4.5031667|   16.9|   21.8|       No|          No|2008|   12|  1|            14.0|                0.0|               6.0|               7.0|              0.0|(1,[0],[1.0])| (15,[7],[1.0])| (15,[6],[1.0])| (15,[0],[1.0])|(48,[14],[1.0])|(110,[14,48,69,85...|\n",
      "|  Albury|    7.4|   25.1|                0.0|1.8671489542210176|7.624853|        WNW|3.8066624897703196|       NNW|       WSW|1.6094379124341003|        22.0|       44.0|       25.0|     1010.6|     1007.8|4.4371896|4.5031667|   17.2|   24.3|       No|          No|2008|   12|  2|            14.0|                9.0|               9.0|               3.0|              0.0|(1,[0],[1.0])| (15,[3],[1.0])| (15,[9],[1.0])| (15,[9],[1.0])|(48,[14],[1.0])|(110,[14,57,72,81...|\n",
      "|  Albury|   12.9|   25.7|                0.0|1.8671489542210176|7.624853|        WSW|3.8501476017100584|         W|       WSW| 2.995732273553991|        26.0|       38.0|       30.0|     1007.6|     1008.7|4.4371896|      2.0|   21.0|   23.2|       No|          No|2008|   12|  3|            14.0|                6.0|               6.0|               3.0|              0.0|(1,[0],[1.0])| (15,[3],[1.0])| (15,[6],[1.0])| (15,[6],[1.0])|(48,[14],[1.0])|(110,[14,54,69,81...|\n",
      "|  Albury|    9.2|   28.0|                0.0|1.8671489542210176|7.624853|         NE|3.2188758248682006|        SE|         E|2.4849066497880004|         9.0|       45.0|       16.0|     1017.6|     1012.8|4.4371896|4.5031667|   18.1|   26.5|       No|          No|2008|   12|  4|            14.0|               13.0|               1.0|              10.0|              0.0|(1,[0],[1.0])|(15,[10],[1.0])| (15,[1],[1.0])|(15,[13],[1.0])|(48,[14],[1.0])|(110,[14,61,64,88...|\n",
      "|  Albury|   17.5|   32.3| 0.6931471805599453|1.8671489542210176|7.624853|          W|3.7376696182833684|       ENE|        NW|2.0794415416798357|        20.0|       82.0|       33.0|     1010.8|     1006.0|      7.0|      8.0|   17.8|   29.7|       No|          No|2008|   12|  5|            14.0|                0.0|              10.0|               8.0|              0.0|(1,[0],[1.0])| (15,[8],[1.0])|(15,[10],[1.0])| (15,[0],[1.0])|(48,[14],[1.0])|(110,[14,48,73,86...|\n",
      "|  Albury|   14.6|   29.7| 0.1823215592774815|1.8671489542210176|7.624853|        WNW|  4.04305126783455|         W|         W| 2.995732273553991|        24.0|       55.0|       23.0|     1009.2|     1005.4|4.4371896|4.5031667|   20.6|   28.9|       No|          No|2008|   12|  6|            14.0|                9.0|               6.0|               1.0|              0.0|(1,[0],[1.0])| (15,[1],[1.0])| (15,[6],[1.0])| (15,[9],[1.0])|(48,[14],[1.0])|(110,[14,57,69,79...|\n",
      "|  Albury|   14.3|   25.0|                0.0|1.8671489542210176|7.624853|          W|3.9318256327243257|        SW|         W| 3.044522437723423|        24.0|       49.0|       19.0|     1009.6|     1008.2|      1.0|4.5031667|   18.1|   24.6|       No|          No|2008|   12|  7|            14.0|                0.0|               7.0|               1.0|              0.0|(1,[0],[1.0])| (15,[1],[1.0])| (15,[7],[1.0])| (15,[0],[1.0])|(48,[14],[1.0])|(110,[14,48,70,79...|\n",
      "|  Albury|    7.7|   26.7|                0.0|1.8671489542210176|7.624853|          W|  3.58351893845611|       SSE|         W|1.9459101490553132|        17.0|       48.0|       19.0|     1013.4|     1010.1|4.4371896|4.5031667|   16.3|   25.5|       No|          No|2008|   12|  8|            14.0|                0.0|               3.0|               1.0|              0.0|(1,[0],[1.0])| (15,[1],[1.0])| (15,[3],[1.0])| (15,[0],[1.0])|(48,[14],[1.0])|(110,[14,48,66,79...|\n",
      "|  Albury|    9.7|   31.9|                0.0|1.8671489542210176|7.624853|        NNW| 4.394449154672439|        SE|        NW|2.0794415416798357|        28.0|       42.0|        9.0|     1008.9|     1003.6|4.4371896|4.5031667|   18.3|   30.2|       No|         Yes|2008|   12|  9|            14.0|               14.0|               1.0|               8.0|              0.0|(1,[0],[1.0])| (15,[8],[1.0])| (15,[1],[1.0])|(15,[14],[1.0])|(48,[14],[1.0])|(110,[14,62,64,86...|\n",
      "|  Albury|   13.1|   30.1| 0.8754687274197924|1.8671489542210176|7.624853|          W| 3.367295829986474|         S|       SSE| 2.772588722239781|        11.0|       58.0|       27.0|     1007.0|     1005.7|4.4371896|4.5031667|   20.1|   28.2|      Yes|          No|2008|   12| 10|            14.0|                0.0|               5.0|               5.0|              1.0|    (1,[],[])| (15,[5],[1.0])| (15,[5],[1.0])| (15,[0],[1.0])|(48,[14],[1.0])|(110,[14,48,68,83...|\n",
      "|  Albury|   13.4|   30.4|                0.0|1.8671489542210176|7.624853|          N|3.4339872044851463|       SSE|       ESE|2.8903717578961645|         6.0|       48.0|       22.0|     1011.8|     1008.7|4.4371896|4.5031667|   20.4|   28.8|       No|         Yes|2008|   12| 11|            14.0|                3.0|               3.0|               9.0|              0.0|(1,[0],[1.0])| (15,[9],[1.0])| (15,[3],[1.0])| (15,[3],[1.0])|(48,[14],[1.0])|(110,[14,51,66,87...|\n",
      "|  Albury|   15.9|   21.7| 1.1631508247068418|1.8671489542210176|7.624853|        NNE|3.4657359027997265|        NE|       ENE| 2.772588722239781|        13.0|       89.0|       91.0|     1010.5|     1004.2|      8.0|      8.0|   15.9|   17.0|      Yes|         Yes|2008|   12| 12|            14.0|               15.0|              12.0|              14.0|              1.0|    (1,[],[])|(15,[14],[1.0])|(15,[12],[1.0])|     (15,[],[])|(48,[14],[1.0])|(110,[14,75,92,94...|\n",
      "|  Albury|   15.9|   18.6| 2.8094027183426014|1.8671489542210176|7.624853|          W| 4.127134385045092|       NNW|       NNW| 3.367295829986474|        28.0|       76.0|       93.0|      994.3|      993.0|      8.0|      8.0|   17.4|   15.8|      Yes|         Yes|2008|   12| 13|            14.0|                0.0|               9.0|              13.0|              1.0|    (1,[],[])|(15,[13],[1.0])| (15,[9],[1.0])| (15,[0],[1.0])|(48,[14],[1.0])|(110,[14,48,72,91...|\n",
      "|  Albury|   12.6|   21.0| 1.5260562827629987|1.8671489542210176|7.624853|         SW|3.8066624897703196|         W|       SSW|3.2188758248682006|        20.0|       65.0|       43.0|     1001.2|     1001.8|4.4371896|      7.0|   15.8|   19.8|      Yes|          No|2008|   12| 14|            14.0|                7.0|               6.0|              12.0|              1.0|    (1,[],[])|(15,[12],[1.0])| (15,[6],[1.0])| (15,[7],[1.0])|(48,[14],[1.0])|(110,[14,55,69,90...|\n",
      "|  Albury|    9.8|   27.7| 1.2089526310090144|1.8671489542210176|7.624853|        WNW|3.9318256327243257|         N|       WNW|2.7081827530467932|        22.0|       50.0|       28.0|     1013.4|     1010.3|      0.0|4.5031667|   17.3|   26.2|       No|          No|2008|   12| 16|            14.0|                9.0|               0.0|               7.0|              0.0|(1,[0],[1.0])| (15,[7],[1.0])| (15,[0],[1.0])| (15,[9],[1.0])|(48,[14],[1.0])|(110,[14,57,63,85...|\n",
      "|  Albury|   14.1|   20.9|                0.0|1.8671489542210176|7.624853|        ENE|3.1354942159291497|       SSW|         E|2.4849066497880004|         9.0|       69.0|       82.0|     1012.2|     1010.4|      8.0|      1.0|   17.2|   18.1|       No|         Yes|2008|   12| 17|            14.0|               11.0|              13.0|              10.0|              0.0|(1,[0],[1.0])|(15,[10],[1.0])|(15,[13],[1.0])|(15,[11],[1.0])|(48,[14],[1.0])|(110,[14,59,76,88...|\n",
      "|  Albury|   13.5|   22.9| 2.8791984144362717|1.8671489542210176|7.624853|          W|4.1588830833596715|         N|       WNW|1.9459101490553132|        20.0|       80.0|       65.0|     1005.8|     1002.2|      8.0|      1.0|   18.0|   21.5|      Yes|         Yes|2008|   12| 18|            14.0|                0.0|               0.0|               7.0|              1.0|    (1,[],[])| (15,[7],[1.0])| (15,[0],[1.0])| (15,[0],[1.0])|(48,[14],[1.0])|(110,[14,48,63,85...|\n",
      "|  Albury|   11.2|   22.5| 2.4510051309976397|1.8671489542210176|7.624853|        SSE| 3.784189633918261|       WSW|        SW|3.2188758248682006|        17.0|       47.0|       32.0|     1009.4|     1009.7|4.4371896|      2.0|   15.5|   21.0|      Yes|          No|2008|   12| 19|            14.0|                4.0|              15.0|               4.0|              1.0|    (1,[],[])| (15,[4],[1.0])|     (15,[],[])| (15,[4],[1.0])|(48,[14],[1.0])|(110,[14,52,82,94...|\n",
      "|  Albury|    9.8|   25.6|                0.0|1.8671489542210176|7.624853|        SSE| 3.295836866004329|        SE|       NNW|2.8903717578961645|         6.0|       45.0|       26.0|     1019.2|     1017.1|4.4371896|4.5031667|   15.8|   23.2|       No|          No|2008|   12| 20|            14.0|                4.0|               1.0|              13.0|              0.0|(1,[0],[1.0])|(15,[13],[1.0])| (15,[1],[1.0])| (15,[4],[1.0])|(48,[14],[1.0])|(110,[14,52,64,91...|\n",
      "|  Albury|   11.5|   29.3|                0.0|1.8671489542210176|7.624853|          S|3.2188758248682006|        SE|        SE| 2.302585092994046|         9.0|       56.0|       28.0|     1019.3|     1014.8|4.4371896|4.5031667|   19.1|   27.3|       No|          No|2008|   12| 21|            14.0|                5.0|               1.0|               0.0|              0.0|(1,[0],[1.0])| (15,[0],[1.0])| (15,[1],[1.0])| (15,[5],[1.0])|(48,[14],[1.0])|(110,[14,53,64,78...|\n",
      "+--------+-------+-------+-------------------+------------------+--------+-----------+------------------+----------+----------+------------------+------------+-----------+-----------+-----------+-----------+---------+---------+-------+-------+---------+------------+----+-----+---+----------------+-------------------+------------------+------------------+-----------------+-------------+---------------+---------------+---------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "updated_feats = [feat+\"_ohe\" for feat in categorical_feats]\n",
    "updated_feats += numeric_feats\n",
    "assembler = VectorAssembler(inputCols=updated_feats, outputCol='features')\n",
    "output = assembler.transform(encoded)\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the target variable RainTomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|RainTomorrow|\n",
      "+------------+\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "|           1|\n",
      "|           0|\n",
      "|           1|\n",
      "|           1|\n",
      "|           1|\n",
      "|           0|\n",
      "|           0|\n",
      "|           1|\n",
      "|           1|\n",
      "|           0|\n",
      "|           0|\n",
      "|           0|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = output.withColumn('RainTomorrow', when(df['RainTomorrow']==\"Yes\", 1).otherwise(0))\n",
    "output.select('RainTomorrow').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training data and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = output.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol='features',labelCol='RainTomorrow')\n",
    "lrmodel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrmodel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.846365\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "labelCol=\"RainTomorrow\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "td = df\n",
    "td = td.withColumn('RainTomorrow', when(df['RainTomorrow']==\"Yes\", 1).otherwise(0))\n",
    "(trainingData, testData) = td.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.849754\n"
     ]
    }
   ],
   "source": [
    "ML_stages = []\n",
    "for feat in categorical_feats:\n",
    "# Todo 1: Using StringIndexer to convert categorical values into category indices for each of the categorical features\n",
    "# add a line of code here\n",
    "    stringIndexer = StringIndexer(inputCol=feat, outputCol=feat+\"_indexed\")\n",
    "    ML_stages.append(stringIndexer)\n",
    "    \n",
    "# Todo 2: Using OneHotEncoder to map a column of category indices to a column of binary vectors for the categorical features.\n",
    "# add a line of code here\n",
    "encoder = OneHotEncoder(inputCols=[feat+\"_indexed\" for feat in categorical_feats], outputCols=[feat+\"_ohe\" for feat in categorical_feats])\n",
    "\n",
    "# Todo 3: Using VectorAssembler, a feature transformer, to merge the columns created in step 9.2.3 and the columns of numerical features into a vector column.\n",
    "# add a line of code here\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "updated_feats = [feat+\"_ohe\" for feat in categorical_feats]\n",
    "updated_feats += numeric_feats\n",
    "assembler = VectorAssembler(inputCols=updated_feats, outputCol='features')\n",
    "\n",
    "# Todo 4: Initialize a Logistic Regression model by LogisticRegression() function using the feature vector generated in Step 10.1.2.\n",
    "# add a line of code here\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol='features',labelCol='RainTomorrow')\n",
    "\n",
    "# Todo 5: Create a pipeline with the stringIndexers, one hot encoder, vector assembler, logistic regression model\n",
    "# add a line of code here\n",
    "ML_stages.extend([encoder, assembler,lr])\n",
    "ML_pipeline = Pipeline(stages = ML_stages)\n",
    "\n",
    "# Todo 6: Fit the pipeline to training dataset, trainingData\n",
    "# add a line of code here\n",
    "lrmodel = ML_pipeline.fit(trainingData)\n",
    "\n",
    "# Todo 7: Make predictions on testing dataset, testData, and put the predictions results into the variable, prediction\n",
    "# add a line of code here\n",
    "predictions = lrmodel.transform(testData)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "labelCol=\"RainTomorrow\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
